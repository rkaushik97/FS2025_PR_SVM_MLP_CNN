{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used code provided by Bastian Alexander Grossenbacher for the Machine Learning Lecture at UNIFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path().resolve().parents[0]\n",
    "train_data_path = project_root / \"data\" / \"fashion_mnist\" / \"train\"\n",
    "test_data_path = project_root / \"data\" / \"fashion_mnist\" / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.2860, Std: 0.3530\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "images, _ = next(iter(loader))\n",
    "mean = images.mean()\n",
    "std = images.std()\n",
    "\n",
    "print(f\"Mean: {mean.item():.4f}, Std: {std.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 60000\n",
      "Train subset size: 48000\n",
      "Val subset size: 12000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean,), (std,))\n",
    "])\n",
    "\n",
    "# Load custom MNIST dataset from folders\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=train_data_path,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Optional: set seed for reproducibility\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "# keep_fraction = 0.3\n",
    "# keep_size = int(keep_fraction * len(dataset))\n",
    "# discard_size = len(dataset) - keep_size\n",
    "# dataset, _ = random_split(dataset, [keep_size, discard_size], generator=generator)\n",
    "\n",
    "\n",
    "# Set split sizes (adjust ratio as needed)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_subset, val_subset = random_split(dataset, [train_size, val_size], generator=generator)\n",
    "print(\"Full dataset size:\", len(dataset))\n",
    "print(\"Train subset size:\", len(train_subset))\n",
    "print(\"Val subset size:\", len(val_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self, num_channels=1, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_channels,\n",
    "                out_channels=32,\n",
    "                kernel_size=2,\n",
    "                stride=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=2,\n",
    "                stride=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, out_channels=128, kernel_size=3, stride=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(1),\n",
    "            nn.Dropout(),\n",
    "            nn.LazyLinear(num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4778, Accuracy: 82.83%\n",
      "Epoch [2/10], Loss: 0.3492, Accuracy: 87.54%\n",
      "Epoch [3/10], Loss: 0.3085, Accuracy: 88.79%\n",
      "Epoch [4/10], Loss: 0.2857, Accuracy: 89.64%\n",
      "Epoch [5/10], Loss: 0.2617, Accuracy: 90.65%\n",
      "Epoch [6/10], Loss: 0.2426, Accuracy: 91.01%\n",
      "Epoch [7/10], Loss: 0.2318, Accuracy: 91.53%\n",
      "Epoch [8/10], Loss: 0.2165, Accuracy: 92.08%\n",
      "Epoch [9/10], Loss: 0.2109, Accuracy: 92.31%\n",
      "Epoch [10/10], Loss: 0.1971, Accuracy: 92.84%\n",
      "Test Accuracy: 91.91%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_subset, batch_size=batch_size, shuffle= True)\n",
    "val_loader = DataLoader(dataset=val_subset, batch_size=batch_size, shuffle= True)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((mean), (std)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model = FashionCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predicted = scores.argmax(1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        accuracy = 100.0 * correct / total\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\"  # noqa\n",
    "        )\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in val_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "\n",
    "            predicted = outputs.argmax(1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "        print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
